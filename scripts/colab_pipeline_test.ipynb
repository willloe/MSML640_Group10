{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad731da1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import textwrap\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "REPO_URL = \"https://github.com/willloe/MSML640_Group10.git\"\n",
    "REPO_DIR = \"/content/MSML640_Group10\"\n",
    "BRANCH = \"feature/LoRA\"\n",
    "\n",
    "if not pathlib.Path(REPO_DIR).exists():\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
    "else:\n",
    "    print(\"Repo already present; pulling latest...\")\n",
    "    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\", \"--ff-only\"], check=True)\n",
    "\n",
    "subprocess.run([\"git\", \"-C\", REPO_DIR, \"fetch\", \"origin\", BRANCH], check=True)\n",
    "subprocess.run([\"git\", \"-C\", REPO_DIR, \"checkout\", BRANCH], check=True)\n",
    "subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\", \"--ff-only\"], check=True)\n",
    "\n",
    "SRC = f\"{REPO_DIR}/packages/diffusion/src\"\n",
    "if SRC not in sys.path:\n",
    "    sys.path.append(SRC)\n",
    "\n",
    "import torch, pkgutil\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "import importlib\n",
    "diffusers = importlib.import_module(\"diffusers\")\n",
    "print(\"diffusers version:\", getattr(diffusers, \"__version__\", \"unknown\"))\n",
    "\n",
    "# print(\"\\nSmoke_sdxl_load\")\n",
    "# subprocess.run([\"python\", f\"{REPO_DIR}/scripts/smoke_sdxl_load.py\"], check=False)\n",
    "\n",
    "# print(\"\\nSmoke_synthetic\")\n",
    "# subprocess.run([\"python\", f\"{REPO_DIR}/scripts/smoke_synthetic.py\"], check=False)\n",
    "\n",
    "# print(\"\\nSmoke_infer (base)\")\n",
    "# subprocess.run([\"python\", f\"{REPO_DIR}/scripts/smoke_infer.py\"], check=False)\n",
    "\n",
    "# print(\"\\nSmoke_infer (controlnet edge, dpmpp)\")\n",
    "# subprocess.run([\n",
    "#     \"python\", f\"{REPO_DIR}/scripts/smoke_infer.py\",\n",
    "#     \"--use_controlnet\", \"1\",\n",
    "#     \"--control_from\", \"edge\",\n",
    "#     \"--scheduler\", \"dpmpp\",\n",
    "#     \"--steps\", \"14\",\n",
    "#     \"--guidance\", \"6.0\",\n",
    "#     \"--width\", \"1024\",\n",
    "#     \"--height\", \"768\",\n",
    "#     \"--seed\", \"1234\",\n",
    "# ], check=False)\n",
    "\n",
    "# print(\"\\nSmoke_upscale_inpaint\")\n",
    "# subprocess.run([\"python\", f\"{REPO_DIR}/scripts/smoke_upscale_inpaint.py\"], check=False)\n",
    "\n",
    "LORA_STYLE_DIR = Path(REPO_DIR) / \"data\" / \"lora_style\"\n",
    "LORA_STYLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "outputs_dir = Path(REPO_DIR) / \"outputs\"\n",
    "pngs = sorted(outputs_dir.rglob(\"*.png\"), key=lambda p: p.stat().st_mtime) if outputs_dir.exists() else []\n",
    "copied = 0\n",
    "for p in reversed(pngs):\n",
    "    try:\n",
    "        dest = LORA_STYLE_DIR / p.name\n",
    "        shutil.copy2(p, dest)\n",
    "        copied += 1\n",
    "        if copied >= 12:\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if copied == 0:\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    w, h = 768, 768\n",
    "    xs = np.linspace(0.0, 1.0, w, dtype=np.float32)\n",
    "    ys = np.linspace(0.0, 1.0, h, dtype=np.float32)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "\n",
    "    palettes = [\n",
    "        ((24, 58, 150),  (197, 219, 255)),\n",
    "        ((9, 96, 121),   (232, 246, 249)),\n",
    "        ((32, 32, 32),   (200, 200, 200)),\n",
    "        ((106, 27, 154), (236, 224, 246)),\n",
    "        ((20, 97, 48),   (216, 240, 223)),\n",
    "        ((120, 54, 35),  (244, 229, 224)),\n",
    "    ]\n",
    "\n",
    "    def to_img(c0, c1, alpha, vignette=0.12, noise_amp=0.02, seed=0):\n",
    "        a = np.clip(alpha, 0.0, 1.0)\n",
    "\n",
    "        rng = np.random.default_rng(seed)\n",
    "        fx, fy = rng.uniform(0.4, 0.8), rng.uniform(0.4, 0.8)\n",
    "        phase = rng.uniform(0, 2*np.pi)\n",
    "        tex = np.sin(2*np.pi*(fx*X + fy*Y) + phase) * noise_amp\n",
    "\n",
    "        R = np.sqrt((X - 0.5)**2 + (Y - 0.5)**2)\n",
    "        vig = 1.0 - vignette * (R / R.max())**2\n",
    "\n",
    "        a = np.clip(a + tex, 0.0, 1.0) * vig\n",
    "\n",
    "        c0 = np.array(c0, dtype=np.float32)[None, None, :]\n",
    "        c1 = np.array(c1, dtype=np.float32)[None, None, :]\n",
    "        img = c0*(1.0 - a[..., None]) + c1*(a[..., None])\n",
    "        return Image.fromarray(np.clip(img, 0, 255).astype(np.uint8))\n",
    "\n",
    "    seeds = [11, 22, 33, 44, 55, 66]\n",
    "\n",
    "    alpha0 = X\n",
    "    alpha1 = (X*np.cos(np.deg2rad(45)) + Y*np.sin(np.deg2rad(45)))\n",
    "    alpha2 = Y\n",
    "    alpha3 = (X*np.cos(np.deg2rad(135)) + Y*np.sin(np.deg2rad(135)))\n",
    "    alpha4 = np.sqrt((X - 0.5)**2 + (Y - 0.5)**2)\n",
    "    alpha4 = (alpha4 / alpha4.max())\n",
    "    alpha5 = np.sqrt((X - 0.35)**2 + (Y - 0.6)**2)\n",
    "    alpha5 = (alpha5 / alpha5.max())\n",
    "\n",
    "    alphas = [alpha0, alpha1, alpha2, alpha3, alpha4, alpha5]\n",
    "\n",
    "    for i in range(6):\n",
    "        c0, c1 = palettes[i % len(palettes)]\n",
    "        img = to_img(c0, c1, alphas[i], seed=seeds[i])\n",
    "        out = LORA_STYLE_DIR / f\"synthetic_{i:02d}.png\"\n",
    "        img.save(out)\n",
    "\n",
    "    copied = 6\n",
    "\n",
    "print(f\"LoRA Style set prepared with {copied} images at: {LORA_STYLE_DIR}\")\n",
    "print(\"Sample files:\", [p.name for p in list(LORA_STYLE_DIR.glob('*.png'))[:3]])\n",
    "\n",
    "RUN_DIR = Path(REPO_DIR) / \"outputs\" / \"lora\" / \"runs\" / \"exp01\"\n",
    "MANIFESTS = Path(RUN_DIR) / \"manifests\"\n",
    "MANIFESTS.mkdir(parents=True, exist_ok=True)\n",
    "CAP_JSONL = MANIFESTS / \"captions.jsonl\"\n",
    "\n",
    "print(\"\\nLoRA prepare_lora_dataset.py\")\n",
    "subprocess.run([\n",
    "    \"python\", f\"{REPO_DIR}/scripts/prepare_lora_dataset.py\",\n",
    "    \"--images_dir\", f\"{LORA_STYLE_DIR}\",\n",
    "    \"--out_jsonl\", f\"{CAP_JSONL}\",\n",
    "    \"--fallback_caption\", \"soft abstract gradient, slide-safe, minimal clutter\"\n",
    "], check=False)\n",
    "\n",
    "lines = CAP_JSONL.read_text().strip().splitlines() if CAP_JSONL.exists() else []\n",
    "if len(lines) == 0:\n",
    "    print(\"LoRA prepare_lora_dataset.py produced no lines; writing fallback captions.jsonl...\")\n",
    "    CAP_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with CAP_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for p in sorted(LORA_STYLE_DIR.glob(\"*.png\")):\n",
    "            f.write(json.dumps({\"image\": str(p), \"caption\": \"soft abstract gradient, slide-safe, minimal clutter\"}) + \"\\n\")\n",
    "        for p in sorted(LORA_STYLE_DIR.glob(\"*.jpg\")):\n",
    "            f.write(json.dumps({\"image\": str(p), \"caption\": \"soft abstract gradient, slide-safe, minimal clutter\"}) + \"\\n\")\n",
    "    lines = CAP_JSONL.read_text().strip().splitlines()\n",
    "\n",
    "print(f\"LoRA captions.jsonl lines: {len(lines)}\")\n",
    "assert len(lines) > 0, \"captions.jsonl is empty; style set may be empty.\"\n",
    "\n",
    "print(\"\\nLoRA train_lora.py (short run)\")\n",
    "\n",
    "env = os.environ.copy()\n",
    "SRC = f\"{REPO_DIR}/packages/diffusion/src\"\n",
    "env[\"PYTHONPATH\"] = SRC + \":\" + env.get(\"PYTHONPATH\", \"\")\n",
    "\n",
    "\n",
    "# cp = subprocess.run(\n",
    "#     [\n",
    "#         \"python\", \"-u\", f\"{REPO_DIR}/scripts/train_lora.py\",\n",
    "#         \"--images_dir\", f\"{LORA_STYLE_DIR}\",\n",
    "#         \"--output_dir\", f\"{RUN_DIR}\",\n",
    "#         \"--resolution\", \"512\",\n",
    "#         \"--rank\", \"8\",\n",
    "#         \"--batch_size\", \"1\",\n",
    "#         \"--gradient_accumulation_steps\", \"1\",\n",
    "#         \"--max_train_steps\", \"20\",\n",
    "#         \"--train_jsonl\", f\"{CAP_JSONL}\",\n",
    "#         \"--checkpoint_steps\", \"10\",\n",
    "#     ],\n",
    "#     check=False,\n",
    "#     text=True,\n",
    "#     capture_output=True,\n",
    "#     env=env,\n",
    "# )\n",
    "\n",
    "# print(\"=== train_lora.py STDOUT ===\")\n",
    "# print(cp.stdout)\n",
    "# print(\"=== train_lora.py STDERR ===\")\n",
    "# print(cp.stderr)\n",
    "# print(\"train_lora.py return code:\", cp.returncode)\n",
    "\n",
    "FINAL_LORA_DIR = Path(RUN_DIR) / \"final_lora\"\n",
    "print(\"LoRA final_lora exists:\", FINAL_LORA_DIR.exists(),\n",
    "      \"contents:\", list(FINAL_LORA_DIR.glob(\"*\")))\n",
    "\n",
    "assert FINAL_LORA_DIR.exists(), \"final_lora directory was not created.\"\n",
    "\n",
    "lora_files = list(FINAL_LORA_DIR.glob(\"*.pt\"))\n",
    "assert lora_files, \"No LoRA .pt file found in final_lora (expected unet_lora_peft.pt).\"\n",
    "\n",
    "print(\"Using LoRA file:\", lora_files[0])\n",
    "AB_OUT = Path(REPO_DIR) / \"outputs\" / \"lora_ab\"\n",
    "AB_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nLoRA smoke_lora_ab.py (A/B compare)\")\n",
    "\n",
    "cp2 = subprocess.run(\n",
    "    [\n",
    "        \"python\", \"-u\", f\"{REPO_DIR}/scripts/smoke_lora_ab.py\",\n",
    "        \"--lora_dir\", f\"{FINAL_LORA_DIR}\",\n",
    "        \"--out_dir\", f\"{AB_OUT}\",\n",
    "        \"--seed\", \"777\",\n",
    "        \"--width\", \"512\",   # 512x512 to be T4-friendly\n",
    "        \"--height\", \"512\",\n",
    "        \"--steps\", \"20\",\n",
    "        \"--guidance\", \"5.5\",\n",
    "        \"--control_mode\", \"safe\",\n",
    "    ],\n",
    "    check=False,\n",
    "    text=True,\n",
    "    capture_output=True,\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "print(\"=== smoke_lora_ab.py STDOUT ===\")\n",
    "print(cp2.stdout)\n",
    "print(\"=== smoke_lora_ab.py STDERR ===\")\n",
    "print(cp2.stderr)\n",
    "print(\"smoke_lora_ab.py return code:\", cp2.returncode)\n",
    "\n",
    "if cp2.returncode != 0:\n",
    "    raise AssertionError(\"smoke_lora_ab.py failed; see stderr above.\")\n",
    "\n",
    "base_imgs = list(AB_OUT.glob(\"ab_seed*_base.png\"))\n",
    "lora_imgs = list(AB_OUT.glob(\"ab_seed*_lora.png\"))\n",
    "print(\"[A/B] base images:\", base_imgs)\n",
    "print(\"[A/B] lora images:\", lora_imgs)\n",
    "assert len(base_imgs) > 0 and len(lora_imgs) > 0, \"A/B images not found. Check smoke_lora_ab output.\"\n",
    "\n",
    "print(\"\\nCompleted. Check /content/MSML640_Group10/outputs for images, including LoRA A/B outputs.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
