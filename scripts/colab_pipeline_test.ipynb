{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad731da1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import textwrap\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "REPO_URL = \"https://github.com/willloe/MSML640_Group10.git\"\n",
    "REPO_DIR = \"/content/MSML640_Group10\"\n",
    "BRANCH = \"feature/LoRA\"\n",
    "\n",
    "if not pathlib.Path(REPO_DIR).exists():\n",
    "    subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], check=True)\n",
    "else:\n",
    "    print(\"Repo already present; pulling latest...\")\n",
    "    subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\", \"--ff-only\"], check=True)\n",
    "\n",
    "subprocess.run([\"git\", \"-C\", REPO_DIR, \"fetch\", \"origin\", BRANCH], check=True)\n",
    "subprocess.run([\"git\", \"-C\", REPO_DIR, \"checkout\", BRANCH], check=True)\n",
    "subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\", \"--ff-only\"], check=True)\n",
    "\n",
    "SRC = f\"{REPO_DIR}/packages/diffusion/src\"\n",
    "if SRC not in sys.path:\n",
    "    sys.path.append(SRC)\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "print(\"\\nSmoke_sdxl_load\")\n",
    "subprocess.run([\"python\", f\"{REPO_DIR}/scripts/smoke_sdxl_load.py\"], check=False)\n",
    "\n",
    "print(\"\\nSmoke_synthetic\")\n",
    "subprocess.run([\"python\", f\"{REPO_DIR}/scripts/smoke_synthetic.py\"], check=False)\n",
    "\n",
    "print(\"\\nSmoke_infer (base)\")\n",
    "subprocess.run([\"python\", f\"{REPO_DIR}/scripts/smoke_infer.py\"], check=False)\n",
    "\n",
    "print(\"\\nSmoke_infer (controlnet edge, dpmpp)\")\n",
    "subprocess.run([\n",
    "    \"python\", f\"{REPO_DIR}/scripts/smoke_infer.py\",\n",
    "    \"--use_controlnet\", \"1\",\n",
    "    \"--control_from\", \"edge\",\n",
    "    \"--scheduler\", \"dpmpp\",\n",
    "    \"--steps\", \"14\",\n",
    "    \"--guidance\", \"6.0\",\n",
    "    \"--width\", \"1024\",\n",
    "    \"--height\", \"768\",\n",
    "    \"--seed\", \"1234\",\n",
    "], check=False)\n",
    "\n",
    "print(\"\\nSmoke_upscale_inpaint\")\n",
    "subprocess.run([\"python\", f\"{REPO_DIR}/scripts/smoke_upscale_inpaint.py\"], check=False)\n",
    "\n",
    "LORA_STYLE_DIR = Path(REPO_DIR) / \"data\" / \"lora_style\"\n",
    "LORA_STYLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "outputs_dir = Path(REPO_DIR) / \"outputs\"\n",
    "pngs = sorted(outputs_dir.rglob(\"*.png\"), key=lambda p: p.stat().st_mtime) if outputs_dir.exists() else []\n",
    "copied = 0\n",
    "for p in reversed(pngs):\n",
    "    try:\n",
    "        dest = LORA_STYLE_DIR / p.name\n",
    "        shutil.copy2(p, dest)\n",
    "        copied += 1\n",
    "        if copied >= 12:\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nLoRA: prepared style set with {copied} images at {LORA_STYLE_DIR}\")\n",
    "\n",
    "lora_jsonl = Path(REPO_DIR) / \"outputs\" / \"lora\" / \"runs\" / \"exp01\" / \"manifests\" / \"captions.jsonl\"\n",
    "lora_jsonl.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nLoRA: prepare_lora_dataset\")\n",
    "subprocess.run([\n",
    "    \"python\", f\"{REPO_DIR}/scripts/prepare_lora_dataset.py\",\n",
    "    \"--images_dir\", str(LORA_STYLE_DIR),\n",
    "    \"--out_jsonl\", str(lora_jsonl),\n",
    "    \"--fallback_caption\", \"soft abstract gradient, slide-safe, minimal clutter\"\n",
    "], check=False)\n",
    "\n",
    "print(\"\\nLoRA: train_lora (short run)\")\n",
    "subprocess.run([\n",
    "    \"python\", f\"{REPO_DIR}/scripts/train_lora.py\",\n",
    "    \"--images_dir\", str(LORA_STYLE_DIR),\n",
    "    \"--output_dir\", f\"{REPO_DIR}/outputs/lora/runs/exp01\",\n",
    "    \"--resolution\", \"768\",\n",
    "    \"--rank\", \"8\",\n",
    "    \"--batch_size\", \"1\",\n",
    "    \"--gradient_accumulation_steps\", \"1\",\n",
    "    \"--max_train_steps\", \"20\"\n",
    "], check=False)\n",
    "\n",
    "final_lora_dir = Path(REPO_DIR) / \"outputs\" / \"lora\" / \"runs\" / \"exp01\" / \"final_lora\"\n",
    "ab_out = Path(REPO_DIR) / \"outputs\" / \"lora_ab\"\n",
    "ab_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nLoRA: smoke_lora_ab (A/B compare)\")\n",
    "subprocess.run([\n",
    "    \"python\", f\"{REPO_DIR}/scripts/smoke_lora_ab.py\",\n",
    "    \"--lora_dir\", str(final_lora_dir),\n",
    "    \"--out_dir\", str(ab_out),\n",
    "    \"--seed\", \"777\",\n",
    "    \"--width\", \"768\",\n",
    "    \"--height\", \"768\",\n",
    "    \"--steps\", \"20\",\n",
    "    \"--guidance\", \"5.5\",\n",
    "    \"--control_mode\", \"safe\"\n",
    "], check=False)\n",
    "\n",
    "print(\"\\nCompleted. Check /content/MSML640_Group10/outputs for images, including LoRA A/B outputs.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
